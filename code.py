# -*- coding: utf-8 -*-
"""MNIST Digit Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10jdEArsNcKmHkBePH7cYfrMaAElzlGHo
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/MNIST Data.zip"

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import random
import sklearn.datasets as ds
import matplotlib.pyplot as plt
import math
import sys
import glob
import time
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.image as mpimg

# %matplotlib inline
import seaborn as sns

def MeanSubtraction(array):
    a = np.mean(array)
    return np.subtract(array,a)

def loadDataset(path):
    print('Loading Dataset...')

    train_x, train_y, test_x, test_y = [], [], [], []

    for i in range(10):
        for filename in glob.glob(path + '/training/' + str(i)+'/*.png'):
            im = mpimg.imread(filename)
            train_x.append(im)
            train_y.append(i)
        for filename in glob.glob(path + '/testing/' + str(i)+'/*.png'):
            im=mpimg.imread(filename)
            test_x.append(im)
            test_y.append(i)

    print('Dataset loaded...')

    return np.array(train_x), np.array(train_y), np.array(test_x),np.array(test_y)

train_x, train_y, test_x, test_y = loadDataset('/content/MNIST Data')

plt.imshow(train_x[5000])
plt.show()

train_x = MeanSubtraction(train_x)

plt.imshow(train_x[5000])

def TsnePlot(train_x,train_y):
    import pandas as pd
    feat_cols = ['pixel' + str(i) for i in range(train_x.shape[1])]
    df = pd.DataFrame(train_x, columns=feat_cols)
    df['y'] = train_y
    df['label'] = df['y'].apply(lambda i: str(i))
    X, y = None, None
    print('Size of the dataframe: {}'.format(df.shape))
    np.random.seed(42)
    rndperm = np.random.permutation(df.shape[0])


    N = 10000
    df_subset = df.loc[rndperm[:N],:].copy()
    data_subset = df_subset[feat_cols].values
    pca = PCA(n_components=3)
    pca_result = pca.fit_transform(data_subset)

    df_subset['pca-one'] = pca_result[:,0]
    df_subset['pca-two'] = pca_result[:,1]
    df_subset['pca-three'] = pca_result[:,2]

    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

    pca_50 = PCA(n_components=50)
    pca_result_50 = pca_50.fit_transform(data_subset)
    time_start = time.time()
    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
    tsne_results = tsne.fit_transform(pca_result_50)

    df_subset['tsne-2d-one'] = tsne_results[:,0]
    df_subset['tsne-2d-two'] = tsne_results[:,1]
    plt.figure(figsize=(16,10))

    sns.scatterplot(
        x="tsne-2d-one", y="tsne-2d-two",
        hue="y",
        palette=sns.color_palette("hls", 10),
        data=df_subset,
        legend="full",
        alpha=0.3
    )

train_x = np.reshape(train_x,(train_x.shape[0],train_x.shape[1]*train_x.shape[2]))
test_x = np.reshape(test_x,(test_x.shape[0],test_x.shape[1]*test_x.shape[2]))

TsnePlot(train_x,train_y)

c = np.zeros((train_y.shape[0],10), dtype=int)

for j in range(len(c)):
    c[j][train_y[j]] = 1

train_y = c
d = np.zeros((test_y.shape[0],10), dtype=int)

for j in range(len(d)):
    d[j][test_y[j]] = 1
test_y = d
test_x = MeanSubtraction(test_x)

print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)

from sklearn.utils import shuffle

train_xx,train_yy = shuffle(train_x,train_y)
test_x,test_y = shuffle(test_x,test_y)

train_pct_index = int(0.90 * len(train_xx))

train_x = train_xx[:train_pct_index]
train_y = train_yy[:train_pct_index]

validX = train_xx[train_pct_index:]
validY = train_yy[train_pct_index:]

train_x = np.append(train_x,np.ones([len(train_x), 1]), 1)
test_x = np.append(test_x,np.ones([len(test_x), 1]), 1)
validX = np.append(validX,np.ones([len(validX), 1]), 1)

class Neural_Network(object):
    def __init__(self,layers=2,inputSize = 784,neurons=[128, 64, 10]):
        self.inputSize = inputSize
        self.layers = layers
        self.neuron = neurons
        self.X1=None
        self.X2=None

        self.W1 = np.random.randn(inputSize+1,neurons[0])*np.sqrt(2/(inputSize+1+neurons[0]))
        self.W2 = np.random.randn(neurons[0]+1,neurons[1])*np.sqrt(2/(neurons[0]+1+neurons[1]))
        self.W3 = np.random.randn(neurons[1]+1,neurons[2])*np.sqrt(2/(neurons[1]+1+neurons[2]))

    def sigmoid(self, s):
        return 1/(1+np.exp(-s))

    def sigmoid_derivative(self, s):
        return s*(1-s)

    def softmax(self, s):
        exps=np.exp(s-np.max(s))

        return exps/np.sum(exps,axis=1,keepdims=True)

    def feedforward(self, X):
        self.X1=np.dot(X,self.W1)
        self.X1=self.sigmoid(self.X1)
        self.X1=np.append(self.X1,np.ones([len(self.X1),1]),1)
        self.X2=np.dot(self.X1,self.W2)
        self.X2=self.sigmoid(self.X2)
        self.X2=np.append(self.X2,np.ones([len(self.X2),1]),1)

        X3=np.dot(self.X2,self.W3)
        X3=self.softmax(X3)

        return X3

    def crossentropy(self, Y, Y_pred):
        return -(np.sum(Y*np.log(Y_pred+np.exp(-12))))/len(Y)

    def backwardpropagate(self,X, Y, y_pred, lr):
        Err=self.crossentropy(Y,y_pred)
        derror=y_pred-Y
        ss=np.dot(self.X2.transpose(),derror)
        ss=lr*ss

        ss1=np.dot(self.W3,derror.transpose())
        ss1=ss1.transpose()*self.sigmoid_derivative(self.X2)
        ss3=np.dot(self.X1.transpose(),ss1)
        ss3=ss3[:,:-1]
        ss3=lr*ss3

        ss2=ss1[:,:-1]
        ss2=np.dot(self.W2,ss2.transpose())
        ss2=ss2.transpose()*self.sigmoid_derivative(self.X1)
        ss2=np.dot(X.transpose(),ss2)
        ss2=ss2[:,:-1]
        ss2=lr*ss2

        self.W1=self.W1-ss2
        self.W2=self.W2-ss3
        self.W3=self.W3-ss

        return np.sum(Err)

    def predict(self, testX):
        return self.feedforward(testX)

    def accuracy(self, testX, testY):
        prediction=self.predict(testX)
        predict=np.argmax(prediction, axis = 1)
        groundtruth=np.argmax(testY, axis = 1)

        acc=0

        for i in range(len(predict)):
            if(predict[i]==groundtruth[i]):
                acc=acc+1
        acc=(acc/len(predict))*100

        return acc

    def confusion(self,testX,testY):
        prediction=self.predict(testX)
        predict=np.argmax(prediction, axis = 1)
        groundtruth=np.argmax(testY, axis = 1)
        Matrix=np.zeros((10,10),dtype=int)

        for j in range(len(groundtruth)):
            Matrix[groundtruth[j]][predict[j]]=Matrix[groundtruth[j]][predict[j]]+1

        return Matrix

    def SGD(self, X, Y, learningRate , batchSize):
        losses = []

        for i in range(0, X.shape[0], batchSize):
            batchX = X[i : i + batchSize]
            batchy = Y[i : i + batchSize]
            y_pred = self.feedforward(batchX)
            loss = self.crossentropy(batchy, y_pred)
            self.backwardpropagate(batchX, batchy, y_pred, learningRate)

        return loss

    def train(self, train_set_x, train_set_y, test_x,test_y, learning_rate, batch_size, training_epochs):
        plotting=list()
        accuracys=list()
        valplot=list()
        valacc=list()

        for j in range(training_epochs):
            tloss=self.SGD( train_set_x, train_set_y, learning_rate , batch_size)
            plotting.append(tloss)
            accuracys.append(self.accuracy(train_set_x,train_set_y))
            testloss=self.crossentropy(test_y,self.feedforward(test_x))
            valplot.append(testloss)
            valacc.append(self.accuracy(test_x,test_y))
            print(f'Epoch {j+1} : Training Loss = {tloss:.4f}, Training Accuracy = {self.accuracy(train_set_x, train_set_y):.4f}, Testing Loss = {testloss:.4f}, Testing Accuracy = {self.accuracy(test_x, test_y):.4f}')

        plt.plot(plotting)
        plt.plot(valplot)
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.show()
        plt.plot(accuracys)
        plt.plot(valacc)
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.show()

    def saveModel(self, name):
        np.savez(name+'.npz', name1=self.W1, name2=self.W2,name3=self.W3)

    def loadModel(self, name):
        data = np.load(name+'.npz')
        self.W1=data['name1']
        self.W2=data['name2']
        self.W3=data['name3']

model = Neural_Network(2, 784, [128, 64, 10])

model.train(train_x, train_y, test_x, test_y, 0.0001, 64, 150)

model.confusion(train_x, train_y)

model.confusion(validX, validY)

model.confusion(test_x, test_y)

model.accuracy(train_x, train_y)

model.accuracy(test_x, test_y)

model.accuracy(validX, validY)

model.saveModel('model')